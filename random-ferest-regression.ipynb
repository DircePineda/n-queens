{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Data array\nX = np.array([8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])\ny = np.array([92, 352, 724, 2680, 14200, 73712, 365596, 2279184, 14772512, 95815104, 666090624,\n              4968057848, 39029188884, 314666222712, 2691008701644, 24233937684440, 227514171973736,\n              2207893435808350, 22317699616364000, 234907967154122000])\n\n# Reshape X to 2D array\nX = X.reshape(-1, 1)\n\n# Split data: training & testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train Random Forest Regression model\nrf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_regressor.fit(X_train, y_train)\n\n# Predictions on test set\ny_pred = rf_regressor.predict(X_test)\n\n# Evaluate model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n\n# Plot data and Random Forest regression curve\nX_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny_plot = rf_regressor.predict(X_plot)\n\nplt.scatter(X, y, label=\"Data\")\nplt.plot(X_plot, y_plot, color='red', label='Random Forest Regression')\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.title(\"Random Forest Regression\")\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-03T18:01:32.367286Z","iopub.execute_input":"2023-10-03T18:01:32.367907Z","iopub.status.idle":"2023-10-03T18:01:35.138936Z","shell.execute_reply.started":"2023-10-03T18:01:32.367872Z","shell.execute_reply":"2023-10-03T18:01:35.137400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RANDOM FOREST REGRESSION\n# 1) Random Sampling of Data\n# Bootstrap Sampling: create multiple subsets of training data through bootstrap sampling\n    # Formula: Randomly select n data points with replacement from training dataset, where n = size of training dataset\n    \n# 2) Random Subset of Features\n# Random Feature Selection: constructe each decision tree, randomly select a subset of features for consideration at each split. Introduces randomness & decorrelates trees\n    # Formula: Randomly select m features from total p features at each split, where m = typically << p\n\n# 3) Decision Tree Construction\n# Decision Tree Construction: For each subset of data (obtained by bootstrap sampling) & each node in decision tree:\n    # Find best feature among randomly selected features (Formula 2) to split data\n    # Calculate impurity or error measure (mean squared error for regression) for potential splits\n    # Choose feature that results in lowest impurity or error measure\n    # Split data based on this feature\n    # Continue recursively until a stopping criterion is met (maximum tree depth, minimum samples per leaf)\n    \n# 4) Aggregation of Predictions\n# Aggregation of Predictions: add from each decision tree. Regression tasks, most common aggregation method is averaging\n    # Formula: Calculate predicted value from each tree (Formula 3) & average these predictions to obtain final prediction\n# Final Prediction (for regression):\n    # y_pred_final = (1/N) * Î£ y_pred_i\n    # N = number of decision trees in forest\n    # y_pred_i = predicted value from i-th decision tree","metadata":{},"execution_count":null,"outputs":[]}]}