{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Data array\nX = np.array([8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27])\ny = np.array([92, 352, 724, 2680, 14200, 73712, 365596, 2279184, 14772512, 95815104, 666090624,\n              4968057848, 39029188884, 314666222712, 2691008701644, 24233937684440, 227514171973736,\n              2207893435808350, 22317699616364000, 234907967154122000])\n\n# Reshape X to 2D array\nX = X.reshape(-1, 1)\n\n# Split data: training & testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train SVR model\nsvr = SVR(kernel='rbf', C=1e3, gamma=0.1)\nsvr.fit(X_train, y_train)\n\n# Predictions on test set\ny_pred = svr.predict(X_test)\n\n# Evaluate model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n\n# Plot data and SVR regression curve\nX_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\ny_plot = svr.predict(X_plot)\n\nplt.scatter(X, y, label=\"Data\")\nplt.plot(X_plot, y_plot, color='red', label='SVR Regression')\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.title(\"SVR Regression\")\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-03T15:03:38.967601Z","iopub.execute_input":"2023-10-03T15:03:38.968204Z","iopub.status.idle":"2023-10-03T15:03:41.080789Z","shell.execute_reply.started":"2023-10-03T15:03:38.968166Z","shell.execute_reply":"2023-10-03T15:03:41.079154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SUPPORT VECTOR REGRESSION (SVR) \n# Extends the concepts of Support Vector Machines (SVMs) from classification to regression problems. \n# modeling both linear and nonlinear relationships between input variables (features) & continuous target variable (output)\n\n# minimize the prediction error while allowing for some tolerance ε\n# minimize: (1/2) * ||w||^2 + C * Σ [ε + ξ_i - ξ_i*]\n# subject to:\n    # y - f(x) <= ε + ξ_i*\n    # f(x) - y <= ε + ξ_i\n    # ξ_i, ξ_i* >= 0\n\n# w represents the weights of features\n# C is a regularization parameter that controls the trade-off between maximizing the margin & minimizing prediction error\n# ξ_i & ξ_i* are slack variables that allow some points to fall outside the margin or make errors within a certain tolerance ε\n# ε is the acceptable error threshold\n\n# Create & train the SVR model. Initialize awith RBF kernel (radial basis function kernel) & hyperparameters (C & gamma)\n# svr = SVR(kernel='rbf', C=1e3, gamma=0.1)   &   svr.fit(X_train, y_train)\n","metadata":{},"execution_count":null,"outputs":[]}]}